{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture Week 3-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "## I. Regression -- Fitting for Two Parameters\n",
    "\n",
    "### &nbsp; &nbsp; &nbsp; &nbsp;  A) Uniform Variance\n",
    "### &nbsp; &nbsp; &nbsp; &nbsp;  B). Non-uniform Variance\n",
    "### &nbsp; &nbsp; &nbsp; &nbsp;  C) $\\rm{\\chi}^2$ Fitting\n",
    "### &nbsp; &nbsp; &nbsp; &nbsp;  D) Reduced $\\rm{\\chi}^2$\n",
    "\n",
    "   \n",
    "## II. Probability and $\\chi^2$ \n",
    "\n",
    "### &nbsp; &nbsp; &nbsp; &nbsp;  A) $\\chi^2$, PDF, and CDF \n",
    "\n",
    "\n",
    "## Lab: \n",
    "\n",
    "## Parts 1. and 2.: The remaining 2 parts from last time (Week 2-2) \n",
    "\n",
    "## 3. CDF for Data with Non-uniform Variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Regression -- Fitting for Two Parameters:\n",
    "\n",
    "### Let's use a simple example: $x$ is time and $y$ is the temperature.  Our goal is to model how the temperature increases in the morning.  The simplest model for this phenomenon is a straight line.\n",
    "\n",
    "### We say the data now has \"Two Degrees of Freedom (DOF)\".\n",
    "\n",
    "### In the one-parameter problem, the data points (room temperature as measured by $N$ thermometers).  Depending on the actual temperature, all $N$ measurement results can shift up and down together.  That's one degree of freedom.\n",
    "\n",
    "### Now, imagine plotting the the temperature measurements made at different times vs. time.  The points can certainly still shift up and down together, depending on whether it's a hot day or a cold day.  But they can also tilt, depending on how fast the temperature is rising with time.  Hence, two degrees of freedom.\n",
    "\n",
    "### Note: Time usually is considered to be measured with much higher accuracy than temperature.  So we take $x$ as given, and treat $y$ as the measurements (Gaussian random numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the data as a straight line\n",
    "\n",
    "That means we think the true relationship between a set of random numbrs (the temperature measurement results) and time is described by a straight line. \n",
    "\n",
    "The goal is then to find the slope ($m$) and intercept ($b$) that best fit the data.\n",
    "\n",
    "Our starting point last time was Question 2 (Week2-2, $\\S$ II.D)): First calculate \n",
    "\n",
    "$$P(D|T) $$\n",
    "\n",
    "and then maximize $P(D|T)$ against $T$.  The temperature value that maximizes this probability is our best guess for the true temperature.\n",
    "\n",
    "This time we will also start from Question 2, except now there are two parameters, so we calcuate\n",
    "\n",
    "$$P(D|[m, b]) \\tag{1}$$\n",
    "\n",
    "and of course the data will consist of the random numbers:\n",
    "\n",
    "$$D = \\{y_1, y_2, ..., y_N\\} = \\{ y_i\\}$$\n",
    "\n",
    "Then, we vary $[m, b]$ until the probability of having the measured data be $D$ ($= \\{y_i\\}$) is maximized.\n",
    "\n",
    "###*The pair $[m, b]$ that maximizes this probability is our best guess for the true slope and intercept.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To find the best-fit $[m, b]$\n",
    "\n",
    "If you go through the math, you will see that maximizing the probability $P(D|[m, b])$ is the same as *minimizing*\n",
    "\n",
    "\n",
    "$$ \\sum_{i=1}^N {[y_i - f(x_i)]^2}  = \\sum_{i=1}^N {[y_i - (mx_i + b)]^2}  \\tag{2}$$\n",
    "\n",
    "\n",
    "where $f(x) = mx + b$, is the linear model.\n",
    "\n",
    "(Let's think about this result.  It's NOT\n",
    "\n",
    "$$ \\sum_{i=1}^N {[y_i - (mx_i + b)]}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This condition is easy to meet: Plot the $(x_i, y_i)$ in a Cartesian coordinate system and find the \"center of mass\" (CM) of the points -- any line that goes through the CM will satisfy eqn (8).  Because half of the points will be above, and the other half below, such a line.  So we can't find the best-fit line this way.)\n",
    "\n",
    "The expression in (2) is called the residual sum of squares (RSS), and the problem to minimize RSS is called:\n",
    "\n",
    "### *Least-squares fitting* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.A) Uniform Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "''' \n",
    "First simulate data, N pairs of {x_i, y_i} using numpy arrays,\n",
    "\n",
    "with uniform variance.\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 20\n",
    "\n",
    "x = np.linspace(8, 12, N)\n",
    "m_true = 4.2\n",
    "b_true = 35\n",
    "\n",
    "â‰ \n",
    "\n",
    "sig = 1.2\n",
    "\n",
    "# ---> This line is the key: it simulates the temperature measurements\n",
    "y = y_true + np.random.randn(x.shape[0]) * sig\n",
    "\n",
    "# 'x': the plot symbols will be crosses.\n",
    "plt.plot(x, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout Exercise: \n",
    "\n",
    "## Given [m, b] = [2, 50], calculate the RSS.\n",
    "\n",
    "## Start by writing a function f(m, b, x) that returns the model values for y (this is basically the f(x) two cells above), given m, b, and an array, x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 20\n",
    "\n",
    "x = np.linspace(8, 12, N)\n",
    "m_true = 4.2\n",
    "b_true = 35\n",
    "\n",
    "y_true = m_true * x + b_true \n",
    "\n",
    "sig = 1.2\n",
    "\n",
    "# ---> This line is the key: it simulates the temperature measurements\n",
    "y = y_true + np.random.randn(x.shape[0]) * sig\n",
    "\n",
    "# 'x': the plot symbols will be crosses.\n",
    "plt.plot(x, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout Exercise: \n",
    "\n",
    "## Search through a range of m and b values and find the pair that minimizes RSS.  You can make use of the function f you wrote above.\n",
    "\n",
    "## You can start by searching for m in the range of (0, 10) and for b in the range of (0, 100) -- 10 steps in each direction. \n",
    "\n",
    "## At the end you should have the following statements printed.\n",
    "\n",
    "The minimum RSS: xx.xxxx.\n",
    "\n",
    "The best-fit slope and intercept for the simulated data are m = 4.5000 and b = 32.0000.\n",
    "\n",
    "(The input values are m_true = 4.2000 and b_true = 35.000000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout Exercise: \n",
    "\n",
    "## Plot the simulated data and the best-fit line in the same plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To include \"error bars\" in your plot:\n",
    "\n",
    "-  ## Change plt.plot to plt.errorbar; \n",
    "\n",
    "-  ## include yerr =  sig;\n",
    "\n",
    "-  ## also pay attention to how to format the plotting symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A better way to tlotting best fit result\n",
    "plt.errorbar(x, y, yerr = sig, fmt = 'x')\n",
    "plt.plot(x, f(m_best, b_best, x), 'k-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important to note: \n",
    "\n",
    "- ## The size of the error bars is set by the quantity sig -- the $\\sigma$ of the Gaussian, and NOT by how much each point deviates from the best-fit line.  Because rememer, in a real experiment, you don't know m_true and b_true.  The line is plotted according to your *best guess* for the slope and the intercept.\n",
    "\n",
    "- ## What the error bars tell us is that if the true temperature is anywhere within the range of the error bar, the measured value will have at least a 68% chance of being at the location of the cross (x).  \n",
    "\n",
    "- ## We think that it's equivalent of saying there is a 68% chance that the true value of the temperature would be within the range indicated by the error bars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.B) Non-uniform Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "''' \n",
    "Simulating data, N pairs of {x_i, y_i}, using numpy arrays, \n",
    "\n",
    "with non-uniform variance.\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 20\n",
    "\n",
    "x = np.linspace(8, 12, N)\n",
    "m_true = 4.2\n",
    "b_true = 35\n",
    "\n",
    "y_true = m_true * x + b_true\n",
    "\n",
    "# specify fractional error, say 10%, which is what occurs for certain experiment.\n",
    "# If you want to sound like a pro, you say \"a signal-to-noise ratio of 10\". \n",
    "# (or S/N = 10).\n",
    "sig_frac = 0.10\n",
    "sig = y_true * sig_frac\n",
    "\n",
    "# simulate the temperature measurements\n",
    "y = y_true + np.random.randn(x.shape[0]) * sig\n",
    "\n",
    "plt.plot(x, y, 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we do now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.C) $\\chi^2$-square fitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We follow the same recipe, by minimizing the probability in eqn (1) above.  After a bit of math, we get:\n",
    "\n",
    "Insteading of minimizing the expression in (2), now we minimize:\n",
    "\n",
    "$$ \\chi^2 = \\sum_{i=1}^N {\\frac {[y_i - f(x_i)]^2} {\\sigma_i^2}}  = \\sum_{i=1}^N {\\frac{[y_i - (mx_i + b)]^2}  {\\sigma_i^2} } \\tag{3}$$\n",
    "\n",
    "This is no longer called least square fitting, but\n",
    "\n",
    "### $\\chi^2$-square fitting.\n",
    "\n",
    "You can think of this as \"weighted least square fitting\".  And you can see that for uniform variance $\\sigma_i = \\sigma$ eqn (3) is reduced to eqn(2), as it should.\n",
    "\n",
    "But really, even in the case of uniform variance, you should calculate $\\chi^2$; you will see why soon.\n",
    "\n",
    "(*Numerical Recipes 3rd Ed., $\\S$15.1*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Example of chi^2 fitting.\n",
    "\n",
    "Given a pair of (m, b) calculate chi^2.\n",
    "\n",
    "'''\n",
    "\n",
    "m = 3.\n",
    "b = 45\n",
    "\n",
    "def f(m, b, x):\n",
    "    return m*x + b\n",
    "\n",
    "y_model = f(m, b, x)\n",
    "\n",
    "chi2 = ((y - y_model)**2/sig**2).sum()\n",
    "print(\"Chi Square is: {:.4f}\".format(chi2))\n",
    "plt.plot(x, y, 'x')\n",
    "plt.plot(x, f(m, b, x), 'k-')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout Exercise: \n",
    "\n",
    "## Search through a range of m and b values and find the pair that minimizes $\\chi^2$.  You can make use of the function f you wrote above.\n",
    "\n",
    "## You can start by searching for m in the range of (0, 10) and for b in the range of (0, 100) -- 10 steps in each direction. \n",
    "\n",
    "## At the end you should have the following statements printed.\n",
    "\n",
    "\n",
    "The minimum chi^2: xx.xxxx.\n",
    "\n",
    "The best-fit slope and intercept for the simulated data are m = 3.6000 and b = 42.0000.\n",
    "\n",
    "(The input values are m_true = 4.2000 and b_true = 35.000000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Plotting best-fit results\n",
    "\n",
    "'''\n",
    "\n",
    "# ms: marker size\n",
    "plt.errorbar(x, y, yerr = sig, fmt = 'o', ms = 6) \n",
    "plt.plot(x, f(m_best, b_best, x), 'k-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-min Break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.D) Reduced $\\chi^2$\n",
    "\n",
    "## $\\chi^2_{\\nu} = \\frac {\\chi^2} {\\text{DOF}}$\n",
    "\n",
    "## Let\n",
    "\n",
    "## $N$ = number of independent data points, and \n",
    "\n",
    "## $n$ = number of fitting parameters.\n",
    "\n",
    "## Then,\n",
    "\n",
    "## $\\text{DOF} = N - n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagine if you only have two points, you will always be able to find m and b that will pass through these two points perfectly.  But you know that shouldn't happen because there will always be uncertainty.  Yes you can fit a line perfectly through 2 data points, but you are fitting for both the actual trend and the noise.  \n",
    "\n",
    "## This happens more often than you think.  It actually has a name:  Overfitting.\n",
    "\n",
    "## Overfitting results when the number of fitting parameter is comparable to or greater than the number of independent data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Reduced chi^2 = chi^2/DOF\n",
    "\n",
    "Note: For a sample of 20 simulated data points, you may get anything between 0.5 and 1.5.  \n",
    "\n",
    "This is due to the small number of data points.  \n",
    "\n",
    "'''\n",
    "\n",
    "DOF = len(x) - 2\n",
    "chi2_nu = chi2_min/DOF\n",
    "print(\"The reduced chi^2 is {:.4f}\".format(chi2_nu))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout Excercise:\n",
    "\n",
    "## Go back to the example of 20 data points with uniform variance and calculate the reduced $\\chi^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can go back further to the case of fitting for one parameter.  Remeber: 50 thermometers measuring room temperature at the same time.  \n",
    "\n",
    "We can easily calculate $\\chi^2_{\\nu}$.  First of all,\n",
    "\n",
    "$$\\chi^2 = \\sum_{i} { \\frac {[t_i - \\langle t \\rangle]^2} {\\sigma_i^2} } \\tag {4}$$\n",
    "\n",
    "Let $N$ be the number of independent data points; the degree of freedom is:\n",
    "\n",
    "$$ \\text{DOF} = \\nu = N - 1 $$\n",
    "\n",
    "We need to subtract 1 because there is *one* fitting parameter.  Then\n",
    "\n",
    "$$\\chi^2_{\\nu} = \\frac{\\chi^2} {N-1} \\tag {5}$$\n",
    "\n",
    "(Again, in the case of only one data point, it's meaningless to compute the average -- the simplest case of overfitting: a horizontal line can be shifted up and down to pass exactly through any one point!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this simplest of all cases, how do we understand why $\\chi^2_{\\nu}$ should be roughly 1:\n",
    "\n",
    "\n",
    "If our assumption that noise follows a Gaussian distribution is right, then roughly 2/3 of the times each of the terms\n",
    "\n",
    "$$\\frac{ t_i - \\langle t \\rangle} {\\sigma_i}  \\tag{6} $$\n",
    "\n",
    "\n",
    "is less than 1, and 1/3 of the times they are greater than 1.  But in the sum of $\\chi^2$, the terms we are summing is the square of the expression in (6):\n",
    "\n",
    "$$\\left[ \\frac{ t_i - \\langle t \\rangle} {\\sigma_i} \\right]^2 \\tag{7} $$\n",
    "\n",
    "\n",
    "When a number less than 1 is squared, it's even smaller, but there is a lower bound, 0.  When a number larger than 1 is squared, it's even larger, but there is no bound from above.  So even though only ~1/3 of the times will a term in the form of (7) be larger than 1, they have longer \"lever arms\" from the number 1.  If there are a large number of terms, those that are greater than 1 exactly balance those that are less than 1.  Thus the average of all the terms in $\\chi^2$ (eqn (4)) is 1.  This is essentially what the reduced $\\chi^2$ calculates: the average of the terms in the form of (7).  (You may wonder if for computing the average we need $N$ in the denominator of eqn (5) instead of $N-1$.  First of all, if $N$ is large, it makes little difference whether it's $N$ or $N-1$.  For small $N$, well, $N-1$ is correct -- if you actually do the math.).\n",
    "\n",
    "This is not a proof but rather an intuitive way for you understand why reduced $\\chi^2$ should be around 1.\n",
    "\n",
    "One other way of understanding eqn (4) or the expression in (7): We are basically trying to get a sense of how big the deviations are from our best estimate of the temperature.  It makes sense to measure the deviation for each data point in unit of $\\sigma_i$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Probability and $\\chi^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.A) $\\chi^2$, PDF, and CDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember we started this whole discussion wanting to calculate and maximize this probability (again using temperature measurement as the example):\n",
    "\n",
    "## $$P(T|D)\\text{,} \\tag{8}$$\n",
    "\n",
    "## and hope to obtain, given data $D$, the most likely value for the true temperature.\n",
    "\n",
    "## But it turns out we don't know how to calculate the probability in (8).  Instead we calculate and maximize this probability \n",
    "\n",
    "## $$P(D|T)\\text{,} \\tag{9}$$\n",
    "\n",
    "## and obtain the temperature value for which the data $D$ is the most likely. \n",
    "\n",
    "## We then assume the temperature value that maximizes (9) also maximizes (8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For one temperature measurement, the probability is (See Week2-2, $\\S$ II.A)),\n",
    "\n",
    "\n",
    "## $$P(t; T) = \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{ -\\frac{ (t - T)^2 } { 2\\sigma^2 }} \\tag{10}$$\t\n",
    "\n",
    "## For a data set of $N$ measurements,\n",
    "\n",
    "\n",
    "## $P(D|T) = P(t_1, t_2, t_i,..., t_N; T) = P(t_1; T)P(t_2; T)...P(t_i; T)...P(t_N; T)$ \n",
    "## $\\hspace{2.75em} = e^{\\left[ -\\frac{(t_1-T)^2}{2\\sigma^2}-\\frac{(t_2-T)^2}{2\\sigma^2}-...\\frac{(t_i-T)^2}{ 2\\sigma^2}-...-\\frac{(t_N-T)^2}{2\\sigma^2} \\right]}$\n",
    "\n",
    "## Or,\n",
    "\n",
    "## $$P(D|T) = Ae^{-\\chi^2/2} \\text{,} \\tag{11}$$\n",
    "\n",
    "##where $A = \\left( \\frac{1}{\\sqrt{2\\pi\\sigma}}\\right)^N$ (See $\\S$ II.D) of Notebook Week2-2.)\n",
    "\n",
    "## Eqn (11) is the probability of how likely the data set $D = \\{t_i\\}$ is going to occur given the temperture is $T$.  When it is integrated over all possible values for $\\{t_i\\}$, we should get 1, by definition.\n",
    "\n",
    "## But we will now reinterpret this as a probability for $T$ -- remember we are saying $P(D|T)$ is the same as $P(T|D)$ -- so we will replace the L.H.S. of (11) by $P(T|D)$.\n",
    "\n",
    "## For the R.H.S. of (11), what's traditionally done is to divide it by $e^{-\\chi_\\text{min}^2/2}$.  So now we have:\n",
    "\n",
    "## $$P(T|D) \\propto e^{-\\Delta\\chi^2/2} \\text{,} \\tag{12}$$\n",
    "\n",
    "## where $\\Delta\\chi^2 = \\chi^2 - \\chi^2_{\\text{min}}$.\n",
    "\n",
    "## The proportionality constant will be determined by requiring that this probability integrated over all possible values of $T$ is 1.\n",
    "\n",
    "## Computationally, you integrate over a large enough range of $T$ around $T_{best}$ and require the result to be 1.\n",
    "\n",
    "\n",
    "## This an example of one fitting parameter ($T$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the case of 2 fitting parameters:\n",
    "\n",
    "## Given $m$ and $b$, the probability of the $i$th measurement being $y_i$ is\n",
    "\n",
    "## $$\\frac{1}{\\sqrt{2\\pi\\sigma}} e^{\\frac {[y_i - f(x_i)]^2} {\\sigma^2}} = \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{ \\frac{[y_i - (mx_i + b)]^2}{\\sigma^2} }  \\tag{13}$$\n",
    "\n",
    "\n",
    "## and thus, \n",
    "\n",
    "## $P(D|[m, b]) = P(y_1; [m, b])P(y_2; [m, b])...P(y_i; [m, b])...P(y_N; [m, b])$ \n",
    "## $\\hspace{4.3em} = Ae^{\\sum_{i=1}^N {\\frac{[y_i - (mx_i + b)]^2}{2\\sigma^2}}}$\n",
    "\n",
    "## Or,\n",
    "\n",
    "## $$P(D|[m, b]) = Ae^{-\\chi^2/2}\\text{,} \\tag{14}$$\n",
    "\n",
    "## where, again, $A = \\left( \\frac{1}{\\sqrt{2\\pi\\sigma}}\\right)^N$\n",
    "\n",
    "## Eqn (14) is the probability of how likely the data set $D = \\{y_i\\}$ going to occur given the slope and intercept are $[m, b]$.  When it is integrated over all possible values for $\\{y_i\\}$, we should get 1, by definition.\n",
    "\n",
    "## But we will now reinterpret this as a probability for $[m, b]$.\n",
    "\n",
    "## So again, replacing the L.H.S. of (14) by $P([m, b]|D)$ and dividing the R.H.S. by $e^{-\\chi_\\text{min}^2/2}$, we have:\n",
    "\n",
    "## $$P([m, b]|D) \\propto e^{-\\Delta\\chi^2/2} \\text{,} \\tag{15}$$\n",
    "\n",
    "## The proportionality constant will be determined by requiring that this probability integrated over all possible values of $m$ and $b$ is 1.\n",
    "\n",
    "## Computationally, you integrate over a large enough area in the $m\\text{-}b$ space around the point $(m_{best}, b_{best})$ and require the result to be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The take away:\n",
    "\n",
    "## The probability of a model, characterized by a set of parameters, being the correct model for reality is proportional to $e^{-\\Delta\\chi^2/2}$.\n",
    "\n",
    "## For the one-parameter example,\n",
    "\n",
    "## $$P(\\text{ model }|\\text{ data })= P(T|D) \\propto Ae^{-\\Delta\\chi^2/2} .$$\n",
    "\n",
    "## For the two-parameter example, \n",
    "\n",
    "## $$P(\\text{ model }|\\text{ data }) = P([m, b]|D) \\propto e^{-\\Delta\\chi^2/2}.$$\n",
    "\n",
    "## In general,\n",
    "\n",
    "## $$P(\\text{ model }|\\text{ data }) \\propto e^{-\\Delta\\chi^2/2} \\text{.} \\tag{16}$$\n",
    "\n",
    "## If your model has $n$ parameters, $\\text{model}(p_1, p_2,..., p_n)$, the proportionality constant in (16) will be determined by requiring that this probability integrated over all these parameters is 1.\n",
    "\n",
    "\n",
    "## N.B.: The quantity in the exponent in (16) is $\\Delta\\chi^2$ and not $\\Delta\\chi^2_{\\nu}$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## OK, let's do it!\n",
    "\n",
    "## First, the simplest example, a one-parameter model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the example below:\n",
    "\n",
    "-  ## Get Probability Distribution Function (PDF) from $e^{-\\Delta\\chi^2/2}$ .  \n",
    "\n",
    "-  ## We will then calculate the Cumulative Distribution Function, or CDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "'''\n",
    "This is based on the example at the end of Section II of Week2-2.\n",
    "\n",
    "*******************  IMPORTANT  ***************** \n",
    "\n",
    "- The histogram will show the standard deviation of the measurement, \n",
    "  independent of the number of measurements (N) --  the standard deviation \n",
    "  for the simulated data is roughly the same as sigma; in this case, 0.5.\n",
    "  \n",
    "- But you know you can take the mean and it represents a measurement \n",
    "  of the temperature with a much smaller uncertainty: sigma/sqrt(N).  \n",
    "  As you will see, the probability you calculate based on chi^2 reflects \n",
    "  that (because the calculation of chi^2 includes all data points).\n",
    "************************************************** \n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "# The first part is copied from Week 2-2 -- \n",
    "# just setting things up by simluating temperature measurements\n",
    "\n",
    "T_true = 70\n",
    "sigma = 0.5\n",
    "N = 50\n",
    "\n",
    "t = np.random.randn(N)*sigma + T_true\n",
    "t_mean = t.mean()\n",
    "sig_t = np.std(t)\n",
    "\n",
    "print(\"The mean temperature value is {:.3f} deg (expected: {:.3f}).\".format(t_mean, T_true))\n",
    "print(\"The standard deviation of the simulated temperature measurement \\\n",
    "results are {:.3f} deg (expected: {:.3f}).\".format(sig_t, sigma))\n",
    "\n",
    "plt.figure()\n",
    "bin_heights, xbins, _ = plt.hist(t, bins = 7, normed = 1, alpha = 0.5)\n",
    "plt.title('Histogram of {:d} Measurements'.format(N))\n",
    "plt.show()\n",
    "\n",
    "# Now the new stuff: PDF and CDF\n",
    "\n",
    "delT = 0.01\n",
    "print(delT)\n",
    "\n",
    "chi2 = []\n",
    "# I pick a small temperature range, because I know with N = 50, \n",
    "# the PDF will have a small spread. You can verify for yourself\n",
    "# that you can change the breadth of the PDF by varying N.\n",
    "T_lo, T_hi = 69.5, 70.5\n",
    "T_vals = np.arange(T_lo, T_hi, delT)\n",
    "for T in T_vals:\n",
    "    # We could use the input sigma (which can be measured from looking at instrumentation carefully)\n",
    "    # But our simulated data set is simple enough and we can use data itself to estimate the sigma, \n",
    "    # which, of course, is just sig_t.\n",
    "    chi2.append(((t - T)**2/(2.*sig_t**2)).sum())\n",
    "\n",
    "chi2 = np.array(chi2)\n",
    "P = np.exp(-chi2/2.)\n",
    "\n",
    "# normalization of PDF\n",
    "P /= P.sum()\n",
    "plt.figure()\n",
    "plt.plot(T_vals, P)\n",
    "\n",
    "# writing text on the plot\n",
    "plt.text(69.6, 0.07, \"Note: A much narrower distribution than the histogram\")\n",
    "\n",
    "# setting limits for the plot\n",
    "plt.axis([T_lo, T_hi, 0, 0.075])\n",
    "\n",
    "# adding a descriptive title \n",
    "plt.title('Probability Distribution Function (PDF) Based on {:d} Measurements'.format(N))\n",
    "\n",
    "# CDF\n",
    "\n",
    "# Using numpy.cumsum to obtain the cumulative probability.\n",
    "CDF = np.cumsum(P)\n",
    "plt.figure()\n",
    "plt.plot(T_vals, CDF)\n",
    "plt.axis([T_lo, T_hi, 0., 1+1e-2])\n",
    "plt.title('Cumulative Distribution Function (CDF)')\n",
    "plt.show()\n",
    "\n",
    "# sanity check\n",
    "print(\"Maximum Value and the sum of PDF: {}, {}\".format(P.max(), P.sum()))\n",
    "print(\"Maximum Value of CDF\".format(CDF.max()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Breakout Exercise:\n",
    "\n",
    "## First, you may want to copy your class Gaussian from last time to here.\n",
    "\n",
    "\n",
    "## 1. Generate a set of 50 Gaussian random numbers that represent 50 temperture measurements.  The true temperature is 70 and the measurement uncertainty is different for each of the measurements (let's say you used 50 different thermometers to make the 50 measurements).  Suppose the uncertainties are: \n",
    "\n",
    "(0.3, 0.4, 0.15, 0.2, 0.5, 0.6, 0.1, 0.2, 0.15, 0.25,  \n",
    "0.2, 0.14, 0.35, 0.32, 0.23, 0.7, 1., 0.32, 0.51, 0.5,  \n",
    "0.3, 0.24, 0.5, 0.33, 0.51, 0.26, 0.31, 0.42, 0.15, 0.25,  \n",
    "0.13, 0.42, 0.15, 0.2, 0.5, 0.6, 0.1, 0.2, 0.15, 0.25,  \n",
    "0.32, 0.3, 0.2, 0.4, 0.3, 0.65, 1., 0.2, 0.35, 0.15).  \n",
    "\n",
    "\n",
    "## Find the weighted mean of the data, and report it as t_mean.\n",
    "\n",
    "## 2. Repeat 1. above 100 times and report \n",
    "\n",
    "## t_MEAN, sig_t_mean, and sig_t_mean_expt.\n",
    "\n",
    "## Are the values of t_MEAN and sig_t_mean what you would expect?\n",
    "\n",
    "\n",
    "## 3. Find PDF and CDF and plot both for 1. above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70.000300256041541, 0.031315918225822963, 0.031310245441057587)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Gaussian:    \n",
    "    def __init__(self, mean = 0, sigma = 1):\n",
    "        self.mean = mean\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def Calc_P(self, x):\n",
    "        a = 1/(np.sqrt(2.*np.pi)*self.sigma)\n",
    "        P = a*np.exp(-(x - self.mean)**2/(2.*self.sigma**2))\n",
    "        return P\n",
    "\n",
    "sig = np.array([0.3, 0.4, 0.15, 0.2, 0.5, 0.6, 0.1, 0.2, 0.15, 0.25, \\\n",
    "0.2, 0.14, 0.35, 0.32, 0.23, 0.7, 1., 0.32, 0.51, 0.5, \\\n",
    "0.3, 0.24, 0.5, 0.33, 0.51, 0.26, 0.31, 0.42, 0.15, 0.25, \\\n",
    "0.13, 0.42, 0.15, 0.2, 0.5, 0.6, 0.1, 0.2, 0.15, 0.25, \\\n",
    "0.32, 0.3, 0.2, 0.4, 0.3, 0.65, 1., 0.2, 0.35, 0.15])\n",
    "\n",
    "\n",
    "N = 50\n",
    "t_true = 70\n",
    "i = 0\n",
    "t_mean_list = []\n",
    "while i < 1000:\n",
    "    t = np.random.randn(N)*sig + t_true\n",
    "    mean = t.mean()\n",
    "    t_mean = np.sum(t/(sig**2))/np.sum((1/sig**2))\n",
    "    t_mean_list.append(t_mean)\n",
    "    i += 1\n",
    "\n",
    "t_mean_list = np.array(t_mean_list)\n",
    "t_MEAN = t_mean_list.mean()\n",
    "sig_t_mean = t_mean_list.std()\n",
    "sig_t_mean_expt = np.sqrt(1/np.sum(1/sig**2))\n",
    "print(t_MEAN, sig_t_mean, sig_t_mean_expt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Week 3-1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

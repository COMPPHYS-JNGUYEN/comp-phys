{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##The Perceptron\n",
    "\n",
    "The simplest model of a neuron is that it takes a number of inputs, and when the total stimulus reaches a certain threshold, the neuron \"fires\" (sends out an electronic signal to another neuron).\n",
    "\n",
    "Mathematically, we can model it in the following way:\n",
    "\n",
    "$$y=\\sum_{i = 1}^{n} {w_i x_i} = \\vec{w} \\cdot \\vec{x} \\tag {1}$$\n",
    "\n",
    "where the $(x_1, x_2, ..., x_n)$ are the inputs, with different weights $(w_1, w_2, ..., w_n)$ for each of them, and $y$ is the total stimulus.\n",
    "\n",
    "(In coding, $\\vec{w}$ and $\\vec{x}$ will be implemented as numpy arrays.)\n",
    "\n",
    "The weighted sum $y$ of these inputs is then compared with a threshold, sometimes represented by step function:\n",
    "\n",
    "\n",
    "$$z(y) = \\begin{cases} \n",
    "      0 & y\\leq 0 \\\\\n",
    "      1 & y\\gt 0\n",
    "\\end{cases} \\tag {2}$$\n",
    "\n",
    "Really, $z$ is a function of $\\vec{x}$ and the weights, $\\vec{w}$, $z(\\vec{x}, \\vec{w})$.\n",
    "\n",
    "We then compare $z$ with a desired output: $d(\\vec{x})$ (which obviously depends on the input, but not the weights).\n",
    "\n",
    "Depending on how $z$ is different from $d$, we will adjust the weights to move $z$ closer and closer to $d$.  This process is called \"training\" (the weights).\n",
    "\n",
    "This is what we call a *perceptron*.\n",
    "\n",
    "One a perceptron is trained, it can then produce an output given a set of previously unseen inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There is one problem:\n",
    "\n",
    "When the threshold function $z$ is written in the form of (2), $\\vec{x} = 0$ is necessarily at the boundary.  This may or may not be desirable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fix: Introducing the bias\n",
    "\n",
    "$$\n",
    "y=\\sum_{i}^{n} {w_i x_i} + b = \\vec{w} \\cdot \\vec{x} + b\\tag {3}\n",
    "$$\n",
    "\n",
    "where $b$ is the bias.\n",
    "\n",
    "Then, \n",
    "$$\n",
    "z(y) = z(\\vec{x}, \\vec{w}; b) = \\begin{cases}\n",
    "      0 & y\\leq 0 \\\\\n",
    "      1 & y\\gt 0\n",
    "\\end{cases} \\tag{4}$$\n",
    "\n",
    "The bias shifts the (threshold, or decision) boundary away from the origin and is independent on any input value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptually, it's clearer to think of the problem in this way and treat the inputs and the bias distinctly.\n",
    "\n",
    "## However, mathematically (and numerically):\n",
    "\n",
    "*It's more convenient to implement the bias as an addition input with a constant value of 1.  Assign a weight for this (fake) input, then the weight is the same as the bias; $b$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example:\n",
    "\n",
    "## A perceptron that performs the AND operation on two inputs:\n",
    "\n",
    "| $x_1$ | $x_2$ | $z$   |\n",
    "|:-:    |:-:    |:-:    |   \n",
    "|   0   |   0   |   0   |\n",
    "|   0   |   1   |   0   |\n",
    "|   1   |   0   |   0   |\n",
    "|   1   |   1   |   1   |\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You'd think in this case, two inputs, $x_1$ and $x_2$ would be enough...\n",
    "\n",
    "But acutally we need the bias.  Let's implement it as the third input; call it $x_3$ with an associated weight $w_3$.  You don't care about $w_3$ -- it's only there to help you achieve the desired outcome. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# All imports\n",
    "from __future__ import print_function\n",
    "from random import choice\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-breakout:\n",
    "- ## Write a python function that is the step function; call it step_function.\n",
    "- ## Turn it into a lambda function; call it step_fn. \n",
    "- ## Modify your lambda function and call it step_fun so that you can plot it for x between -10 and 10, say."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD7CAYAAACRxdTpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e9f3ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def z(y):\n",
    "    if y <= 0:\n",
    "        z = 0\n",
    "    else:\n",
    "        z = 1\n",
    "    return z\n",
    "\n",
    "step_fn = lambda y: 0 if y<=0 else 1\n",
    "\n",
    "x = np.linspace(-10., 10., 1000)\n",
    "step_fun = lambda y: y>0\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x,step_fun(x))\n",
    "plt.ylim(0., 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "z = 0\n",
    "x = 2 if z==1 else 3\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Perceptron to implement the NOR operation\n",
    "\n",
    "With combined error.\n",
    "\n",
    "'''\n",
    "\n",
    "training_data = [\n",
    "    (np.array([0,0,1]), 1),\n",
    "    (np.array([0,1,1]), 0),\n",
    "    (np.array([1,0,1]), 0),\n",
    "    (np.array([1,1,1]), 0),\n",
    "]\n",
    "\n",
    "# usu. random numbers for weights is not a bad starting point\n",
    "w = np.random.rand(3)\n",
    "errors = []\n",
    "\n",
    "# \"learning rate\"\n",
    "alfa = 0.2\n",
    "\n",
    "# use 100 training steps\n",
    "n = 100\n",
    "# w.history = []\n",
    "for i in xrange(n):\n",
    "    x, target = choice(training_data)\n",
    "    y = np.dot(w, x)\n",
    "    error = target - step_fun(y)\n",
    "    errors.append(error)\n",
    "    w += alfa * error * x\n",
    "\n",
    "print('weights:', w)\n",
    "    \n",
    "for x, _ in training_data:\n",
    "    y = np.dot(x, w)\n",
    "    # not including the bias, x[3]\n",
    "    print(\"{}: {} -> {}\".format(x[:2], y, step_fun(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(n), errors, '.')\n",
    "plt.ylim(-1.1, 1.1)\n",
    "plt.xlim(0, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test = np.array([0.4, 3.0])\n",
    "y = np.dot(x, w)\n",
    "print(\"{}: {} -> {}\".format(x[:2], y, step_fun(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's turn this into a classifier!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Perceptron to implement the AND operation\n",
    "\n",
    "There are lots of subtlties here.\n",
    "\n",
    "\n",
    "'''\n",
    "def NOR_perceptron_classifier(x, show_train = False):    \n",
    "    training_data = [\n",
    "        (np.array([0,0,1]), 1),\n",
    "        (np.array([0,1,1]), 0),\n",
    "        (np.array([1,0,1]), 0),\n",
    "        (np.array([1,1,1]), 0),\n",
    "    ]\n",
    "\n",
    "    # usu. random numbers for weights is not a bad starting point\n",
    "    w = np.random.rand(3)\n",
    "    errors = []\n",
    "\n",
    "    # \"learning rate\"\n",
    "    alfa = 0.2\n",
    "\n",
    "    # use 100 training steps\n",
    "    n = 100\n",
    "    # w.history = []\n",
    "    for i in xrange(n):\n",
    "        x_train, target = choice(training_data)\n",
    "        y = np.dot(w, x_train)\n",
    "        error = target - step_fun(y)\n",
    "        errors.append(error)\n",
    "        w += alfa * error * x_train\n",
    "    \n",
    "    if show_train:\n",
    "        print('weights:', w)\n",
    "        print('Training results:')\n",
    "        for x_train, _ in training_data:\n",
    "            y = np.dot(x_train, w)\n",
    "            print(\"{}: {} -> {}\".format(x_train[:2], y, step_fun(y)))\n",
    "    \n",
    "    x = np.append(x, 1)\n",
    "    return step_fun(np.dot(x, w)), w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z, _ = NOR_perceptron_classifier(np.array([0.5, 1.0]))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout: Try 20 pairs of random numbers between [0, 1]\n",
    "\n",
    "- ## Classify them according their \"z\" value.\n",
    "\n",
    "- ## Plot them, with color coding according their \"z\" value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout: Try 100 pairs of random numbers between [0, 1]\n",
    "\n",
    "- ## Classify them according their \"z\" value.\n",
    "\n",
    "- ## Plot them, with color coding according their \"z\" value.\n",
    "\n",
    "- ## Plot the four training points using large symbols, also color coded according their \"z\" values.  For inspiration:\n",
    "\n",
    "       http://matplotlib.org/api/pyplot_api.html\n",
    "\n",
    "- ## Plot the decision boundary (a line) using w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout: Generate 100 pairs of random numbers between [-10, 10]\n",
    "\n",
    "- ## Classify them according their \"z\" value.\n",
    "\n",
    "- ## Plot them, with color coding according their \"z\" value.\n",
    "\n",
    "- ## Plot the decision boundary (a line) using w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Breakout: A general perceptron classifier:\n",
    "\n",
    "      perceptron_classifier(training_data, x, show_train = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

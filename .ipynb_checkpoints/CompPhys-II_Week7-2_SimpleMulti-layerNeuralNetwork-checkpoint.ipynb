{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Multi-layer Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the different sigmoids:\n",
    "\n",
    "## (https://en.wikipedia.org/wiki/Activation_function, and there are more)\n",
    "\n",
    "Logistic (a.k.a Soft step)\tActivation logistic:\t$f(x)=\\frac{1}{1+e^{-x}}$, \t$\\,f'(x)=f(x)(1-f(x))$,    (0,1)\t\n",
    "\n",
    "TanH\tActivation tanh:\t$f(x)=\\tanh(x)=\\frac{2}{1+e^{-2x}}-1$,\t$\\,f'(x)=1-f(x)^2$,   (-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# All imports\n",
    "\n",
    "# ----------> Note: new import: division! \n",
    "# ----------> try it: print(4/3)\n",
    "from __future__ import print_function, division\n",
    "from random import choice\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(formatter={'float': '{:.4f}'.format})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''The logistic function as the sigmoid'''\n",
    "\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_pr(z):\n",
    "    '''derivative of the logistic function'''\n",
    "    return z*(1-z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout Excercise -- Write a function simple_nn(X, w1, w2) that performs the forward propagation\n",
    "\n",
    "   - ## X is the input array\n",
    "   - ## w1 are the weights in the first layer (it should be a 2x2 array...and think dot product)\n",
    "   - ## w2 are the weights for the second layer\n",
    "   - ## It should return z, b, A, a\n",
    "   - ## Test it with the input values and weights given in lecture.  And then print out the following:\n",
    "   \n",
    " \n",
    "            w1:\n",
    "             [[0.1000 0.4000]\n",
    "             [0.8000 0.6000]]\n",
    "            w2:\n",
    "             [0.3000 0.9000]\n",
    "            a: [0.7550 0.6800]\n",
    "            A: [0.6803 0.6637]\n",
    "            b: 0.801444986674\n",
    "            output (z) of nn: 0.690283492908 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout Excercise -- Write a function training_nn(X, y, w1, w2) that performs the backward propagation\n",
    "\n",
    "   - ## y is the target\n",
    "   - ## Define delta1 as we talked about in lecture\n",
    "   - ## Also for conenience and clarity, define delta_w2 = delta2 * A.  This is essentially how w2 should be adjusted (with alpha = 1).\n",
    "   - ## Then define delta1 as we talked about in class -- think about the best way to do it \n",
    "   - ## Define a delta_w1.   This is essentially how w1 should be adjusted (with alpha = 1). (*Hint*: Think matrix multiplication.)\n",
    "   - ## Run it and you should get this:\n",
    " \n",
    "\n",
    "            w1:\n",
    "             [[0.1000 0.4000]\n",
    "             [0.8000 0.6000]]\n",
    "            w2:\n",
    "             [0.3000 0.9000]\n",
    "            output (z) of nn: 0.690283492908"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Week 7-2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture Week 3-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "\n",
    "## Week 3-2: \n",
    "\n",
    "\n",
    "## I. Regression -- Fitting for Two Parameters (continued)\n",
    "\n",
    "### &nbsp; &nbsp; &nbsp; &nbsp;  D) Reduced $\\rm{\\chi}^2$\n",
    "\n",
    "\n",
    "## II. Probability and $\\chi^2$\n",
    "\n",
    "### &nbsp; &nbsp; &nbsp; &nbsp;  A) $\\chi^2$, Probability Distribution Function (PDF), and Cumulative Distribution Function (CDF)\n",
    "### &nbsp; &nbsp; &nbsp; &nbsp;  B) Confidence Levels\n",
    "\n",
    "\n",
    "\n",
    "## Lab: $\\chi^2$, PDF, CDF, and Confidence Levels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.D) Reduced $\\chi^2$\n",
    "\n",
    "## $\\chi^2_{\\nu} = \\frac {\\chi^2} {\\text{DOF}}$\n",
    "\n",
    "## Let\n",
    "\n",
    "## $N$ = number of independent data points, and \n",
    "\n",
    "## $n$ = number of fitting parameters.\n",
    "\n",
    "## Then,\n",
    "\n",
    "## $\\text{DOF} = N - n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagine if you only have two points, you will always be able to find m and b that will pass through these two points perfectly.  But you know that shouldn't happen because there will always be uncertainty.  Yes you can fit a line perfectly through 2 data points, but you are fitting for both the actual trend and the noise.  \n",
    "\n",
    "## This happens more often than you think.  It actually has a name:  Overfitting.\n",
    "\n",
    "## Overfitting results when the number of fitting parameter is comparable to or greater than the number of independent data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c4ad490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum chi^2:17.6555.\n",
      "The best-fit slope and intercept for the simulated data are m = 4.8000 and b = 30.0000.\n",
      "(The input values are m_true = 4.2000 and b_true = 35.000000).\n",
      "The reduced chi^2 is 0.9809\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "'''\n",
    "Reduced chi^2 = chi^2/DOF\n",
    "\n",
    "Note: For a sample of 20 simulated data points, you may get anything between 0.5 and 1.5.  \n",
    "\n",
    "This is due to the small number of data points.  \n",
    "\n",
    "'''\n",
    "\n",
    "def f(m, b, x):\n",
    "    return m*x + b\n",
    "\n",
    "N = 20\n",
    "x = np.linspace(8, 12, N)\n",
    "\n",
    "# simulate the temperature measurements\n",
    "m_true = 4.2\n",
    "b_true = 35\n",
    "\n",
    "y_true = m_true * x + b_true\n",
    "\n",
    "# specify fractional error, say 10%, which is what occurs \n",
    "#for certain experiment. If you want to sound like a pro, \n",
    "# you say \"a signal-to-noise ratio of 10\". \n",
    "# (or S/N = 10).\n",
    "sig_frac = 0.10\n",
    "sig = y_true * sig_frac\n",
    "\n",
    "y = y_true + np.random.randn(x.shape[0]) * sig\n",
    "\n",
    "\n",
    "\n",
    "m_start, m_end = 0, 10\n",
    "b_start, b_end = 0, 100\n",
    "\n",
    "chi2_min = 1e6\n",
    "for m in np.arange(m_start, m_end, 0.1):\n",
    "    for b in np.arange(b_start, b_end, 1):\n",
    "        chi2 = ((y - f(m, b, x))**2/sig**2).sum()\n",
    "        if chi2 < chi2_min:\n",
    "            chi2_min = chi2\n",
    "            m_best = m\n",
    "            b_best = b\n",
    "\n",
    "# ms: marker size\n",
    "plt.errorbar(x, y, yerr = sig, fmt = 'o', ms = 6) \n",
    "plt.plot(x, f(m_best, b_best, x), 'k-')\n",
    "plt.show()\n",
    "\n",
    "            \n",
    "print('The minimum chi^2:{:.4f}.'.format(chi2_min))\n",
    "print('The best-fit slope and intercept for the simulated data are \\\n",
    "m = {:.4f} and b = {:.4f}.'.format(m_best, b_best))\n",
    "print('(The input values are m_true = {:.4f} and b_true = {:4f}).'.format(m_true, b_true))\n",
    "\n",
    "N = len(x)\n",
    "n = 2\n",
    "\n",
    "DOF = N - n\n",
    "chi2_nu = chi2_min/DOF\n",
    "print(\"The reduced chi^2 is {:.4f}\".format(chi2_nu))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout Excercise:\n",
    "\n",
    "## Go back to the example of 20 data points with uniform variance and calculate the reduced $\\chi^2$\n",
    "\n",
    "## You may start with the following code for simulation and fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Least square minimization with uniform variance: Grid search.\n",
    "\n",
    "Note: for some simulation, I don't exactly get the input values of m_true and b_true.\n",
    "'''\n",
    "N = 20\n",
    "\n",
    "x = np.linspace(8, 12, N)\n",
    "m_true = 4.2\n",
    "b_true = 35\n",
    "\n",
    "y_true = m_true * x + b_true \n",
    "sig = 1.2\n",
    "y = y_true + np.random.randn(x.shape[0]) * sig\n",
    "\n",
    "# Grid search\n",
    "m_start, m_end = 0, 10\n",
    "b_start, b_end = 0, 100\n",
    "\n",
    "rss_min = 1e6\n",
    "for m in np.arange(m_start, m_end, 0.1):\n",
    "    for b in np.arange(0, 100, 1):\n",
    "        rss = ((y - f(m, b, x))**2).sum()\n",
    "        if rss < rss_min:\n",
    "            rss_min = rss\n",
    "            m_best = m\n",
    "            b_best = b\n",
    "            \n",
    "print('The minimum RSS:{:.4f}.'.format(rss_min))\n",
    "print('The best-fit slope and intercept for the simulated data are \\\n",
    "m = {:.4f} and b = {:.4f}.'.format(m_best, b_best))\n",
    "print('(The input values are m_true = {:.4f} and b_true = {:4f}).'.format(m_true, b_true))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can go back further to the case of fitting for one parameter.  Remeber: 50 thermometers measuring room temperature at the same time.  \n",
    "\n",
    "We can easily calculate $\\chi^2_{\\nu}$.  First of all,\n",
    "\n",
    "$$\\chi^2 = \\sum_{i} { \\frac {[t_i - \\langle t \\rangle]^2} {\\sigma_i^2} } \\tag {4}$$\n",
    "\n",
    "Let $N$ be the number of independent data points; the degree of freedom is:\n",
    "\n",
    "$$ \\text{DOF} = \\nu = N - 1 $$\n",
    "\n",
    "We need to subtract 1 because there is *one* fitting parameter.  Then\n",
    "\n",
    "$$\\chi^2_{\\nu} = \\frac{\\chi^2} {N-1} \\tag {5}$$\n",
    "\n",
    "(Again, in the case of only one data point, it's meaningless to compute the average -- the simplest case of overfitting: a horizontal line can be shifted up and down to pass exactly through any one point!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this simplest of all cases, how do we understand why $\\chi^2_{\\nu}$ should be roughly 1:\n",
    "\n",
    "\n",
    "If our assumption that noise follows a Gaussian distribution is right, then roughly 2/3 of the times each of the terms\n",
    "\n",
    "$$\\frac{ t_i - \\langle t \\rangle} {\\sigma_i}  \\tag{6} $$\n",
    "\n",
    "\n",
    "is less than 1, and 1/3 of the times they are greater than 1.  But in the sum of $\\chi^2$, the terms we are summing is the square of the expression in (6):\n",
    "\n",
    "$$\\left[ \\frac{ t_i - \\langle t \\rangle} {\\sigma_i} \\right]^2 \\tag{7} $$\n",
    "\n",
    "\n",
    "When a number less than 1 is squared, it's even smaller, but there is a lower bound, 0.  When a number larger than 1 is squared, it's even larger, but there is no bound from above.  So even though only ~1/3 of the times will a term in the form of (7) be larger than 1, they have longer \"lever arms\" from the number 1.  If there are a large number of terms, those that are greater than 1 exactly balance those that are less than 1.  Thus the average of all the terms in $\\chi^2$ (eqn (4)) is 1.  This is essentially what the reduced $\\chi^2$ calculates: the average of the terms in the form of (7).  (You may wonder if for computing the average we need $N$ in the denominator of eqn (5) instead of $N-1$.  First of all, if $N$ is large, it makes little difference whether it's $N$ or $N-1$.  For small $N$, well, $N-1$ is correct -- if you actually do the math.).\n",
    "\n",
    "This is not a proof but rather an intuitive way for you understand why reduced $\\chi^2$ should be around 1.\n",
    "\n",
    "One other way of understanding eqn (4) or the expression in (7): We are basically trying to get a sense of how big the deviations are from our best estimate of the temperature.  It makes sense to measure the deviation for each data point in unit of $\\sigma_i$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember we started this whole discussion wanting to calculate and maximize this probability (again using temperature measurement as the example):\n",
    "\n",
    "## $$P(T|D)\\text{,} \\tag{8}$$\n",
    "\n",
    "## and hope to obtain, given data $D$, the most likely value for the true temperature.\n",
    "\n",
    "## But it turns out we don't know how to calculate the probability in (8).  Instead we calculate and maximize this probability \n",
    "\n",
    "## $$P(D|T)\\text{,} \\tag{9}$$\n",
    "\n",
    "## and obtain the temperature value for which the data $D$ is the most likely. \n",
    "\n",
    "## We then assume the temperature value that maximizes (9) also maximizes (8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For one temperature measurement, the probability is (See Week2-2, $\\S$ II.A)),\n",
    "\n",
    "\n",
    "## $$P(t; T) = \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{ -\\frac{ (t - T)^2 } { 2\\sigma^2 }} \\tag{10}$$\t\n",
    "\n",
    "## For a data set of $N$ measurements,\n",
    "\n",
    "\n",
    "## $P(D|T) = P(t_1, t_2, t_i,..., t_N; T) = P(t_1; T)P(t_2; T)...P(t_i; T)...P(t_N; T)$ \n",
    "## $\\hspace{2.75em} = e^{\\left[ -\\frac{(t_1-T)^2}{2\\sigma^2}-\\frac{(t_2-T)^2}{2\\sigma^2}-...\\frac{(t_i-T)^2}{ 2\\sigma^2}-...-\\frac{(t_N-T)^2}{2\\sigma^2} \\right]}$\n",
    "\n",
    "## Or,\n",
    "\n",
    "## $$P(D|T) = Ae^{-\\chi^2/2} \\text{,} \\tag{11}$$\n",
    "\n",
    "##where $A = \\left( \\frac{1}{\\sqrt{2\\pi\\sigma}}\\right)^N$ (See $\\S$ II.D) of Notebook Week2-2.)\n",
    "\n",
    "## Eqn (11) is the probability of how likely the data set $D = \\{t_i\\}$ is going to occur given the temperture is $T$.  When it is integrated over all possible values for $\\{t_i\\}$, we should get 1, by definition.\n",
    "\n",
    "## But we will now reinterpret this as a probability for $T$ -- remember we are saying $P(D|T)$ is the same as $P(T|D)$ -- so we will replace the L.H.S. of (11) by $P(T|D)$.\n",
    "\n",
    "## For the R.H.S. of (11), what's traditionally done is to divide it by $e^{-\\chi_\\text{min}^2/2}$.  So now we have:\n",
    "\n",
    "## $$P(T|D) \\propto e^{-\\Delta\\chi^2/2} \\text{,} \\tag{12}$$\n",
    "\n",
    "## where $\\Delta\\chi^2 = \\chi^2 - \\chi^2_{\\text{min}}$.\n",
    "\n",
    "## The proportionality constant will be determined by requiring that this probability integrated over all possible values of $T$ is 1.\n",
    "\n",
    "## Computationally, you integrate over a large enough range of $T$ around $T_{best}$ and require the result to be 1.\n",
    "\n",
    "\n",
    "## This an example of one fitting parameter ($T$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the case of 2 fitting parameters:\n",
    "\n",
    "## Given $m$ and $b$, the probability of the $i$th measurement being $y_i$ is\n",
    "\n",
    "## $$\\frac{1}{\\sqrt{2\\pi\\sigma}} e^{\\frac {[y_i - f(x_i)]^2} {\\sigma^2}} = \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{ \\frac{[y_i - (mx_i + b)]^2}{\\sigma^2} }  \\tag{13}$$\n",
    "\n",
    "\n",
    "## and thus, \n",
    "\n",
    "## $P(D|[m, b]) = P(y_1; [m, b])P(y_2; [m, b])...P(y_i; [m, b])...P(y_N; [m, b])$ \n",
    "## $\\hspace{4.3em} = Ae^{\\sum_{i=1}^N {\\frac{[y_i - (mx_i + b)]^2}{2\\sigma^2}}}$\n",
    "\n",
    "## Or,\n",
    "\n",
    "## $$P(D|[m, b]) = Ae^{-\\chi^2/2}\\text{,} \\tag{14}$$\n",
    "\n",
    "## where, again, $A = \\left( \\frac{1}{\\sqrt{2\\pi\\sigma}}\\right)^N$\n",
    "\n",
    "## Eqn (14) is the probability of how likely the data set $D = \\{y_i\\}$ going to occur given the slope and intercept are $[m, b]$.  When it is integrated over all possible values for $\\{y_i\\}$, we should get 1, by definition.\n",
    "\n",
    "## But we will now reinterpret this as a probability for $[m, b]$.\n",
    "\n",
    "## So again, replacing the L.H.S. of (14) by $P([m, b]|D)$ and dividing the R.H.S. by $e^{-\\chi_\\text{min}^2/2}$, we have:\n",
    "\n",
    "## $$P([m, b]|D) \\propto e^{-\\Delta\\chi^2/2} \\text{,} \\tag{15}$$\n",
    "\n",
    "## The proportionality constant will be determined by requiring that this probability integrated over all possible values of $m$ and $b$ is 1.\n",
    "\n",
    "## Computationally, you integrate over a large enough area in the $m\\text{-}b$ space around the point $(m_{best}, b_{best})$ and require the result to be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The take away:\n",
    "\n",
    "## The probability of a model, characterized by a set of parameters, being the correct model for reality is proportional to $e^{-\\Delta\\chi^2/2}$.\n",
    "\n",
    "## For the one-parameter example,\n",
    "\n",
    "## $$P(\\text{ model }|\\text{ data })= P(T|D) \\propto Ae^{-\\Delta\\chi^2/2} .$$\n",
    "\n",
    "## For the two-parameter example, \n",
    "\n",
    "## $$P(\\text{ model }|\\text{ data }) = P([m, b]|D) \\propto e^{-\\Delta\\chi^2/2}.$$\n",
    "\n",
    "## In general,\n",
    "\n",
    "## $$P(\\text{ model }|\\text{ data }) \\propto e^{-\\Delta\\chi^2/2} \\text{.} \\tag{16}$$\n",
    "\n",
    "## If your model has $n$ parameters, $\\text{model}(p_1, p_2,..., p_n)$, the proportionality constant in (16) will be determined by requiring that this probability integrated over all these parameters is 1.\n",
    "\n",
    "\n",
    "## N.B.: The quantity in the exponent in (16) is $\\Delta\\chi^2$ and not $\\Delta\\chi^2_{\\nu}$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## OK, let's do it!\n",
    "\n",
    "## ... but first a couple of useful numpy functions and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Two numpy array methods: \n",
    "\n",
    "cumsum and concatenate\n",
    "\n",
    "'''\n",
    "a = np.linspace(1, 10, 10)\n",
    "a_cumsum = a.cumsum()\n",
    "print(a)\n",
    "print(a_cumsum)\n",
    "\n",
    "\n",
    "b = np.linspace(-1, -10, 10)\n",
    "# Note: what goes into np.concatenate has to be a tuple\n",
    "c = np.concatenate((a, b))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "plot looks weird because:\n",
    "\n",
    "- Python connects the plotted points according the order of the indpendent variable.\n",
    "- If the x array is not ordered, you will see zig-zag lines.\n",
    "\n",
    "'''\n",
    "\n",
    "# arr = np.random.rand(4)\n",
    "x_arr = np.array([0.9, 0.2, 0.4, 0.5])\n",
    "y_arr = x_arr**2\n",
    "plt.plot(x_arr, y_arr)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''numpy.argsort'''\n",
    "\n",
    "# arr = np.random.rand(4)\n",
    "x_arr = np.array([0.9, 0.2, 0.4, 0.5])\n",
    "y_arr = x_arr**2\n",
    "# sorted_idx tells you the position of each element in the sorted array\n",
    "sorted_idx = np.argsort(x_arr)\n",
    "\n",
    "x_arr_sorted = x_arr[sorted_idx]\n",
    "y_arr_sorted = y_arr[sorted_idx]\n",
    "\n",
    "plt.plot(x_arr_sorted, y_arr_sorted)\n",
    "print('original x_arr: {}'.format(x_arr))\n",
    "print('sorted_idx: {}'.format(sorted_idx))\n",
    "# sorted in ascending order\n",
    "print('sorted x_arr: {}'.format(x_arr_sorted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The idea is that as the array x_arr gets sorted, the indices of the elements are also sorted.\n",
    "\n",
    "## argsort gives you the order of the original indices after the sorting.\n",
    "\n",
    "## Since 0.9 had the original index of 0, and after sorting 0.9 become the last element, so 0 becomes the last element of what argsort returns.\n",
    "\n",
    "## You can then think of sorted_idx (what's returned by argsort) as a set of instructions.  In this case:\n",
    "\n",
    "- Take the original 0th element and put it in the 3rd (last) place\n",
    "- Take the original 1st element and put it in the 0th place\n",
    "- Take the original 2nd element and put it in the 1st place\n",
    "- Take the original 3rd element and put it in the 2nd place.\n",
    "\n",
    "## By doing \n",
    "\n",
    "x_arr_sorted = x_arr[sorted_idx]\n",
    "\n",
    "## you are carrying out this set of instructions.\n",
    "\n",
    "## Similarly by doing \n",
    "\n",
    "y_arr_sorted = y_arr[sorted_idx]\n",
    "\n",
    "## you are sorting y_arr in such a way that each element in y_arr_sorted corresponds to the right element in x_arr_sorted.\n",
    "\n",
    "## You can think of applying argsort to x_arr as a way to get the instructions to sort x_arr, and then you can apply the set of instructions to both x_arr and y_arr so that after sorting, each element in the x array still corresponds to the right element in the y array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Breakout Exercise (one-liner, a bit of a brain teaser...)\n",
    "\n",
    "## If someone gives you an array that has been sorted in ascending order \n",
    "\n",
    "x_arr_sorted = [ 0.2  0.4  0.5  0.9]\n",
    "\n",
    "## and the sorted indices\n",
    "\n",
    "sorted_idx = [1 2 3 0]\n",
    "\n",
    "## How do you get the original array (let's call it arr) back?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before I show you the solution..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When we sorted x_arr into an ascending order, the indices or x_arr got scrambled and are no longer squential.\n",
    "\n",
    "## Now you want to put these indices back in the original order, that is, the sequential order.  Another way of saying that is to order the indices in idx_ordered:\n",
    "\n",
    "##$$ [1\\, 2\\, 3\\, 0]  \\rightarrow [0\\,1\\,2\\,3]$$\n",
    "\n",
    "\n",
    "## Also, note that there is a one-to-one correspondence between sorted_idx and x_arr_sorted (the original index of every element in x_arr_sorted is stored in sorted_idx).\n",
    "\n",
    "\n",
    "##\\begin{array}{llll}\n",
    "[\\mkern-18mu&1 &2 &3 &0]\\\\\n",
    "&\\downarrow &\\downarrow &\\downarrow &\\downarrow \\\\\n",
    "[& 0.2 &0.4 &0.5 &0.9] \\\\\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "## So...let's sort sorted_idx back into sequential order and get the set of instructions to accomplish that. \n",
    "\n",
    "## Well, argsort does exactly that if we apply it to sorted_idx.\n",
    "\n",
    "## Then impose that set of instructions on x_arr_sorted, and you would have gotten the original array back. And we know the way to obtain the instruction is by applying np.argsort.\n",
    "\n",
    "## In summary:\n",
    "\n",
    "## we apply np.argsort to sorted_idx to get the instruction and then impose the instruction on x_arry_sorted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: $\\chi^2$, PDF, CDF, and Confidence Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Breakout Exercise 1:\n",
    "\n",
    "## First, the simplest problem, a one-parameter model:\n",
    "\n",
    "-  ## Simulate 2000 data points -- Gaussian random numbers with mu = 10. and sigma = 2.\n",
    "\n",
    "-  ## Plot histogram.\n",
    "\n",
    "-  ## Calculate chi^2; find the minimum chi^2.\n",
    "\n",
    "-  ## Get Probability Distribution Function (PDF) from $e^{-\\Delta\\chi^2/2}$ .\n",
    "\n",
    "-  ## Plot PDF.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Breakout Exercise 2:\n",
    "\n",
    "- ## Find the Cumulative Distribution Function (CDF) by using the numpy array method .cumsum\n",
    "\n",
    "- ## Then plot the CDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Breakout Exercsise 3\n",
    "-  ## Calculate the \"one-sided\" CDF from the peak of the PDF. \n",
    "-  ## Then use this CDF to find the $1\\text{-}\\sigma$ and $2\\text{-}\\sigma$ of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Week 3-2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

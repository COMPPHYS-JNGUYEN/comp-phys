{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##The Perceptron\n",
    "\n",
    "The simplest model of a neuron is that it takes a number of inputs, and when the total stimulus reaches a certain threshold, the neuron \"fires\" (sends out an electronic signal to another neuron).\n",
    "\n",
    "Mathematically, we can model it in the following way:\n",
    "\n",
    "$$y=\\sum_{i = 1}^{n} {w_i x_i} = \\vec{w} \\cdot \\vec{x} \\tag {1}$$\n",
    "\n",
    "where the $(x_1, x_2, ..., x_n)$ are the inputs, with different weights $(w_1, w_2, ..., w_n)$ for each of them, and $y$ is the total stimulus.\n",
    "\n",
    "(In coding, $\\vec{w}$ and $\\vec{x}$ will be implemented as numpy arrays.)\n",
    "\n",
    "The weighted sum $y$ of these inputs is then compared with a threshold, sometimes represented by step function:\n",
    "\n",
    "\n",
    "$$z(y) = \\begin{cases} \n",
    "      0 & y\\leq 0 \\\\\n",
    "      1 & y\\gt 0\n",
    "\\end{cases} \\tag {2}$$\n",
    "\n",
    "Really, $z$ is a function of $\\vec{x}$ and the weights, $\\vec{w}$, $z(\\vec{x}, \\vec{w})$.\n",
    "\n",
    "We then compare $z$ with a desired output: $d(\\vec{x})$ (which obviously depends on the input, but not the weights).\n",
    "\n",
    "Depending on how $z$ is different from $d$, we will adjust the weights to move $z$ closer and closer to $d$.  This process is called \"training\" (the weights).\n",
    "\n",
    "This is what we call a *perceptron*.\n",
    "\n",
    "One a perceptron is trained, it can then produce an output given a set of previously unseen inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There is one problem:\n",
    "\n",
    "When the threshold function $z$ is written in the form of (2), $\\vec{x} = 0$ is necessarily at the boundary.  This may or may not be desirable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fix: Introducing the bias\n",
    "\n",
    "$$\n",
    "y=\\sum_{i}^{n} {w_i x_i} + b = \\vec{w} \\cdot \\vec{x} + b\\tag {3}\n",
    "$$\n",
    "\n",
    "where $b$ is the bias.\n",
    "\n",
    "Then, \n",
    "$$\n",
    "z(y) = z(\\vec{x}, \\vec{w}; b) = \\begin{cases}\n",
    "      0 & y\\leq 0 \\\\\n",
    "      1 & y\\gt 0\n",
    "\\end{cases} \\tag{4}$$\n",
    "\n",
    "The bias shifts the (threshold, or decision) boundary away from the origin and is independent on any input value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptually, it's clearer to think of the problem in this way and treat the inputs and the bias distinctly.\n",
    "\n",
    "## However, mathematically (and numerically):\n",
    "\n",
    "*It's more convenient to implement the bias as an addition input with a constant value of 1.  Assign a weight for this (fake) input, then the weight is the same as the bias; $b$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example:\n",
    "\n",
    "## A perceptron that performs the AND operation on two inputs:\n",
    "\n",
    "| $x_1$ | $x_2$ | $z$   |\n",
    "|:-:    |:-:    |:-:    |   \n",
    "|   0   |   0   |   0   |\n",
    "|   0   |   1   |   0   |\n",
    "|   1   |   0   |   0   |\n",
    "|   1   |   1   |   1   |\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You'd think in this case, two inputs, $x_1$ and $x_2$ would be enough...\n",
    "\n",
    "But acutally we need the bias.  Let's implement it as the third input; call it $x_3$ with an associated weight $w_3$.  You don't care about $w_3$ -- it's only there to help you achieve the desired outcome. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# All imports\n",
    "from __future__ import print_function\n",
    "from random import choice\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-breakout:\n",
    "- ## Write a python function that is the step function; call it step_function.\n",
    "- ## Turn it into a lambda function; call it step_fn. \n",
    "- ## Modify your lambda function and call it step_fun so that you can plot it for x between -10 and 10, say."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Perceptron to implement the NOR operation\n",
    "\n",
    "With combined error.\n",
    "\n",
    "'''\n",
    "\n",
    "training_data = [\n",
    "    (np.array([0,0,1]), 1),\n",
    "    (np.array([0,1,1]), 0),\n",
    "    (np.array([1,0,1]), 0),\n",
    "    (np.array([1,1,1]), 0),\n",
    "]\n",
    "\n",
    "# usu. random numbers for weights is not a bad starting point\n",
    "w = np.random.rand(3)\n",
    "errors = []\n",
    "\n",
    "# \"learning rate\"\n",
    "alfa = 0.2\n",
    "\n",
    "# use 100 training steps\n",
    "n = 100\n",
    "# w.history = []\n",
    "for i in xrange(n):\n",
    "    x, target = choice(training_data)\n",
    "    y = np.dot(w, x)\n",
    "    error = target - step_fun(y)\n",
    "    errors.append(error)\n",
    "    w += alfa * error * x\n",
    "\n",
    "print('weights:', w)\n",
    "    \n",
    "for x, _ in training_data:\n",
    "    y = np.dot(x, w)\n",
    "    # not including the bias, x[3]\n",
    "    print(\"{}: {} -> {}\".format(x[:2], y, step_fun(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(n), errors, '.')\n",
    "plt.ylim(-1.1, 1.1)\n",
    "plt.xlim(0, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test = np.array([0.4, 3.0])\n",
    "y = np.dot(x, w)\n",
    "print(\"{}: {} -> {}\".format(x[:2], y, step_fun(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's turn this into a classifier!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Perceptron to implement the AND operation\n",
    "\n",
    "There are lots of subtlties here.\n",
    "\n",
    "\n",
    "'''\n",
    "def NOR_perceptron_classifier(x, show_train = False):    \n",
    "    training_data = [\n",
    "        (np.array([0,0,1]), 1),\n",
    "        (np.array([0,1,1]), 0),\n",
    "        (np.array([1,0,1]), 0),\n",
    "        (np.array([1,1,1]), 0),\n",
    "    ]\n",
    "\n",
    "    # usu. random numbers for weights is not a bad starting point\n",
    "    w = np.random.rand(3)\n",
    "    errors = []\n",
    "\n",
    "    # \"learning rate\"\n",
    "    alfa = 0.2\n",
    "\n",
    "    # use 100 training steps\n",
    "    n = 100\n",
    "    # w.history = []\n",
    "    for i in xrange(n):\n",
    "        x_train, target = choice(training_data)\n",
    "        y = np.dot(w, x_train)\n",
    "        error = target - step_fun(y)\n",
    "        errors.append(error)\n",
    "        w += alfa * error * x_train\n",
    "    \n",
    "    if show_train:\n",
    "        print('weights:', w)\n",
    "        print('Training results:')\n",
    "        for x_train, _ in training_data:\n",
    "            y = np.dot(x_train, w)\n",
    "            print(\"{}: {} -> {}\".format(x_train[:2], y, step_fun(y)))\n",
    "    \n",
    "    x = np.append(x, 1)\n",
    "    return step_fun(np.dot(x, w)), w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z, _ = NOR_perceptron_classifier(np.array([0.5, 1.0]))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout: Try 20 pairs of random numbers between [0, 1]\n",
    "\n",
    "- ## Classify them according their \"z\" value.\n",
    "\n",
    "- ## Plot them, with color coding according their \"z\" value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout: Try 100 pairs of random numbers between [0, 1]\n",
    "\n",
    "- ## Classify them according their \"z\" value.\n",
    "\n",
    "- ## Plot them, with color coding according their \"z\" value.\n",
    "\n",
    "- ## Plot the four training points using large symbols, also color coded according their \"z\" values.  For inspiration:\n",
    "\n",
    "       http://matplotlib.org/api/pyplot_api.html\n",
    "\n",
    "- ## Plot the decision boundary (a line) using w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout: Generate 100 pairs of random numbers between [-10, 10]\n",
    "\n",
    "- ## Classify them according their \"z\" value.\n",
    "\n",
    "- ## Plot them, with color coding according their \"z\" value.\n",
    "\n",
    "- ## Plot the decision boundary (a line) using w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Breakout: A general perceptron classifier:\n",
    "\n",
    "      perceptron_classifier(training_data, x, show_train = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
